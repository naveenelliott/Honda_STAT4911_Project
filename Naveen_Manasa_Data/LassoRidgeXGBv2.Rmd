---
title: "Capstone2pt2"
author: "Manasa Subramanian.191"
date: "2025-04-01"
output: html_document
---

```{r}
library(broom)
library(tidyr)
library(R.utils)
library(GGally)
library(ggplot2)
library(MASS)
library(dplyr)
library(foreign)
library(tidyverse)
library(readstata13)
library(haven)
library(ISLR)
library(dplyr)
library(corrplot)
library(survival)
library(knitr)
library(readxl)
library(data.table)
library(haven)
library(Amelia)
library(priceR)
library(randomForest)
library(Metrics)
library(dplyr)
library(caret)
library(glmnet)
```


This is random forest to see importance of variables in comparison to the 
```{r}
set.seed(222)
library(dplyr)
library(randomForest)
library(readr)

# Read and preprocess the training data
train_data <- read_csv("C:/Users/Owner/Downloads/SoccermaticsForPython-master/SoccermaticsForPython-master/Honda_STAT4911_Project/final_previous_merged.csv")
train_data[is.na(train_data)] <- 0
train_data <- train_data %>% 
  select(-c("prev_inf_factor", "prev_X.Other.Currency.to.USD", "prev_financialDate"))

# Read and preprocess the test data
test_data <- read_csv("C:/Users/Owner/Downloads/SoccermaticsForPython-master/SoccermaticsForPython-master/Honda_STAT4911_Project/final_future_merged.csv")
test_data[is.na(test_data)] <- 0
test_data <- test_data %>% 
  select(-c("prev_inf_factor", "prev_X.Other.Currency.to.USD", "prev_financialDate"))

# Check which columns exist in your data to ensure "prev_supplier_number" exists.
# Uncomment the next lines if needed.
# print(names(train_data))
# print(names(test_data))

# Remove "FHR" and "prev_supplier_number" from the predictors
# so that the supplier number is not used during training.
X_train <- train_data %>% select(-FHR)
y_train <- train_data$FHR

# For the test set, do the same removal:
X_test <- test_data %>% select(-prev_Supplier.Number)
y_test <- test_data$FHR

# Train the random forest model using only predictors (excluding the supplier number)
rf_model <- randomForest(x = X_train, y = y_train, ntree = 500, importance = TRUE)

# Make predictions on the test data
y_prediction_rf <- predict(rf_model, newdata = X_test)

# Create a final dataframe that includes the predicted FHR and the supplier number from the test data
final_predictions <- data.frame(
  prev_supplier_number = test_data$prev_Supplier.Number,
  predicted_FHR = y_prediction_rf
)

# Display the final dataframe
print(final_predictions)

write_csv(final_predictions, "predicted_FHR_supplier.csv")
```

RIDGE REGRESSION

```{r}
# Prepare the training and test matrices (excluding the target variable FHR)
X_train <- as.matrix(train_data[, -which(names(train_data) == "FHR")])
y_train <- train_data$FHR 
X_test  <- as.matrix(test_data[, -which(names(test_data) == "FHR")])
y_test  <- test_data$FHR

# Perform cross-validation for ridge regression (alpha = 0 indicates ridge)
CV_ridge <- cv.glmnet(X_train, y_train, alpha = 0)

# Optionally, look at a summary of the CV results (if desired)
print(summary(CV_ridge))

# Plot the cross-validation error vs. lambda values
plot(CV_ridge)

# Identify the best lambda
best_lambda <- CV_ridge$lambda.min
cat("Best lambda:", best_lambda, "\n")

# Predict on the test data using the best lambda
y_predicted_ridge <- predict(CV_ridge, newx = X_test, s = "lambda.min")

# Calculate Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)
mse_ridge <- mean((y_predicted_ridge - y_test)^2)
cat("MSE:", mse_ridge, "\n")
rmse_ridge <- sqrt(mse_ridge)
cat("RMSE:", rmse_ridge, "\n")

# Extract the ridge coefficients at the best lambda value
ridge_coef <- coef(CV_ridge, s = "lambda.min")

# Convert the coefficients to a data frame
ridge_coef_df <- as.data.frame(as.matrix(ridge_coef))
ridge_coef_df$Feature <- rownames(ridge_coef_df)
colnames(ridge_coef_df)[1] <- "Coefficient"

# Remove the intercept (first row) and compute absolute coefficients
ridge_coef_df <- ridge_coef_df[-1, ]
ridge_coef_df$Abs_Coefficient <- abs(ridge_coef_df$Coefficient)

```


#Beginning of LASSO

```{r}
library(glmnet)
finaldf <- read.csv("final_previous_merged.csv")
finaldf[is.na(finaldf)] <- 0
for(col in names(finaldf)) {
  if(any(is.character(finaldf[[col]]))) {
    print(paste("Column", col, "contains character values"))
  }
}
```

#LASSO

```{r}

finaldf_lasso<-finaldf %>% select(-c('prev_X.Other.Currency.to.USD','prev_inf_factor','prev_financialDate'))
```

```{r}
sum(is.na(finaldf))
```


```{r}
set.seed(222)
split <- createDataPartition(finaldf_lasso$FHR, p = 0.8, list = FALSE)

train_data <- finaldf_lasso[split, ]
test_data <- finaldf_lasso[-split, ]

X_train <- as.matrix(train_data[, -which(names(train_data) == "FHR")])
y_train <- train_data$FHR 
X_test <- as.matrix(test_data[, -which(names(test_data) == "FHR")])
y_test <- test_data$FHR
CV_lasso <- cv.glmnet(X_train,y_train,alpha = 1)
summary(CV_lasso)
#plotting the lambdas relation to coefficients
plot(CV_lasso)
print(CV_lasso)
#goodness of fit
best_lambda <- CV_lasso$lambda.min
y_predicted_lasso <- predict(CV_lasso,newx = X_test, s= "lambda.min")
mse_lasso <- (sum((y_predicted_lasso - y_test)^2)/length(y_predicted_lasso))
print(mse_lasso)
rmse_lasso <- sqrt(mse_lasso)
print(rmse_lasso)
lasso_coef <- coef(CV_lasso, s = "lambda.min")
print(lasso_coef)
```

```{r}
set.seed(222)
cat("LASSO RMSE:", rmse_lasso, "\n")
cat("Ridge RMSE:", rmse_ridge, "\n")
```








