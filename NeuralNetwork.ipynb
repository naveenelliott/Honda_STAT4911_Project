{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\owner\\onedrive - the ohio state university\\documents\\anaconda\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\owner\\onedrive - the ohio state university\\documents\\anaconda\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\owner\\onedrive - the ohio state university\\documents\\anaconda\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\owner\\onedrive - the ohio state university\\documents\\anaconda\\lib\\site-packages (from torch) (2022.7.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\owner\\onedrive - the ohio state university\\documents\\anaconda\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\owner\\onedrive - the ohio state university\\documents\\anaconda\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\owner\\onedrive - the ohio state university\\documents\\anaconda\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\owner\\onedrive - the ohio state university\\documents\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\owner\\onedrive - the ohio state university\\documents\\anaconda\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['prev_accountsPayable', 'prev_accountsReceivable',\n",
       "       'prev_bankCashBalances', 'prev_bankOverdraft',\n",
       "       'prev_debitOwnedWithinOneYear', 'prev_financialAssets',\n",
       "       'prev_fixedAssets', 'prev_intangibleAssets', 'prev_otherCurrentAssets',\n",
       "       'prev_otherCurrentLiabilities', 'prev_otherEquity',\n",
       "       'prev_otherTermAssets', 'prev_otherTermLiabilities', 'prev_prepayments',\n",
       "       'prev_retainedEarnings', 'prev_subscribedCapital', 'prev_termLoans',\n",
       "       'prev_totalAssets', 'prev_totalCurrentAssets',\n",
       "       'prev_totalCurrentLiabilities', 'prev_totalInventories',\n",
       "       'prev_totalLiabilities', 'prev_totalShareholderEquity',\n",
       "       'prev_totalTermAssets', 'prev_totalTermLiabilities',\n",
       "       'prev_accountPayableAccrualFromCashFlow', 'prev_capitalExpenditure',\n",
       "       'prev_changeInInventoriesFromCashFlow',\n",
       "       'prev_netChangeInCashAndCashEquivalents', 'prev_netFundingCashFlow',\n",
       "       'prev_netInvestingCashFlow',\n",
       "       'prev_unbilledAccountsReceivableRevenueFromCashFlow',\n",
       "       'prev_abnormalItems', 'prev_amortization', 'prev_companyTaxExpense',\n",
       "       'prev_costOfGoodsSold', 'prev_depreciation',\n",
       "       'prev_earningsBeforeInterestAndTax', 'prev_grossProfit',\n",
       "       'prev_interestExpense', 'prev_interestReceived',\n",
       "       'prev_investmentIncome', 'prev_netInvestmentIncome',\n",
       "       'prev_netProfitAfterTax', 'prev_netProfitAfterTaxAndMinorityInterests',\n",
       "       'prev_netProfitBeforeTax', 'prev_netSurplus', 'prev_otherIncome',\n",
       "       'prev_otherInvestmentExpense', 'prev_otherOperatingExpense',\n",
       "       'prev_totalOperatingExpense', 'prev_totalOperatingRevenue',\n",
       "       'prev_totalStaffCosts', 'prev_salesRevenue', 'prev_eqyYear', 'prev_CHS',\n",
       "       'prev_FHR', 'diff_days', 'FHR', 'dummy_BRL', 'dummy_CAD', 'dummy_CHF',\n",
       "       'dummy_CNY', 'dummy_EUR', 'dummy_GBP', 'dummy_INR', 'dummy_JPY',\n",
       "       'dummy_KRW', 'dummy_MXN', 'dummy_NOK', 'dummy_TWD', 'dummy_USD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('final_previous_merged.csv')\n",
    "\n",
    "df = pd.get_dummies(df, columns=[\"prev_currency\"], prefix=\"dummy\", drop_first=False)\n",
    "\n",
    "df.drop(columns={'prev_X.Other.Currency.to.USD', 'prev_inf_factor', 'prev_financialDate'}, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop the target column for X, keep 'FHR' as y\n",
    "X = df.drop(\"FHR\", axis=1).values  # shape: (num_samples, num_features)\n",
    "y = df[\"FHR\"].values               # shape: (num_samples,)\n",
    "\n",
    "# Split into train and test sets\n",
    "#   test_size=0.2 means 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy arrays to Torch tensors (float)\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FHRModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(FHRModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)          # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Simple forward with ReLU on hidden layer\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_t.shape[1]  # number of features\n",
    "model = FHRModel(input_dim=input_dim, hidden_dim=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/7000], Loss: 140825.7031\n",
      "Epoch [20/7000], Loss: 56494.7930\n",
      "Epoch [30/7000], Loss: 30421.1719\n",
      "Epoch [40/7000], Loss: 7241.1484\n",
      "Epoch [50/7000], Loss: 4366.8164\n",
      "Epoch [60/7000], Loss: 3450.3433\n",
      "Epoch [70/7000], Loss: 2527.0803\n",
      "Epoch [80/7000], Loss: 2038.8750\n",
      "Epoch [90/7000], Loss: 1733.4932\n",
      "Epoch [100/7000], Loss: 1527.1863\n",
      "Epoch [110/7000], Loss: 1341.8009\n",
      "Epoch [120/7000], Loss: 1202.8906\n",
      "Epoch [130/7000], Loss: 1086.2822\n",
      "Epoch [140/7000], Loss: 988.8872\n",
      "Epoch [150/7000], Loss: 903.9279\n",
      "Epoch [160/7000], Loss: 830.0682\n",
      "Epoch [170/7000], Loss: 765.3193\n",
      "Epoch [180/7000], Loss: 708.3344\n",
      "Epoch [190/7000], Loss: 658.0068\n",
      "Epoch [200/7000], Loss: 613.2224\n",
      "Epoch [210/7000], Loss: 573.2981\n",
      "Epoch [220/7000], Loss: 537.6122\n",
      "Epoch [230/7000], Loss: 505.5313\n",
      "Epoch [240/7000], Loss: 476.6058\n",
      "Epoch [250/7000], Loss: 450.5085\n",
      "Epoch [260/7000], Loss: 426.8938\n",
      "Epoch [270/7000], Loss: 405.5860\n",
      "Epoch [280/7000], Loss: 386.2012\n",
      "Epoch [290/7000], Loss: 368.6416\n",
      "Epoch [300/7000], Loss: 352.6750\n",
      "Epoch [310/7000], Loss: 338.1686\n",
      "Epoch [320/7000], Loss: 324.9964\n",
      "Epoch [330/7000], Loss: 313.0451\n",
      "Epoch [340/7000], Loss: 302.1436\n",
      "Epoch [350/7000], Loss: 292.2002\n",
      "Epoch [360/7000], Loss: 283.1247\n",
      "Epoch [370/7000], Loss: 274.8051\n",
      "Epoch [380/7000], Loss: 267.1861\n",
      "Epoch [390/7000], Loss: 260.1919\n",
      "Epoch [400/7000], Loss: 253.7519\n",
      "Epoch [410/7000], Loss: 247.8214\n",
      "Epoch [420/7000], Loss: 242.3629\n",
      "Epoch [430/7000], Loss: 237.3193\n",
      "Epoch [440/7000], Loss: 232.6398\n",
      "Epoch [450/7000], Loss: 228.2864\n",
      "Epoch [460/7000], Loss: 224.2088\n",
      "Epoch [470/7000], Loss: 220.2538\n",
      "Epoch [480/7000], Loss: 216.5277\n",
      "Epoch [490/7000], Loss: 213.0464\n",
      "Epoch [500/7000], Loss: 209.7981\n",
      "Epoch [510/7000], Loss: 206.7563\n",
      "Epoch [520/7000], Loss: 203.8998\n",
      "Epoch [530/7000], Loss: 201.2064\n",
      "Epoch [540/7000], Loss: 198.6685\n",
      "Epoch [550/7000], Loss: 196.2731\n",
      "Epoch [560/7000], Loss: 194.0099\n",
      "Epoch [570/7000], Loss: 191.8659\n",
      "Epoch [580/7000], Loss: 189.8339\n",
      "Epoch [590/7000], Loss: 187.9037\n",
      "Epoch [600/7000], Loss: 186.0697\n",
      "Epoch [610/7000], Loss: 184.2980\n",
      "Epoch [620/7000], Loss: 182.5491\n",
      "Epoch [630/7000], Loss: 180.8748\n",
      "Epoch [640/7000], Loss: 179.2091\n",
      "Epoch [650/7000], Loss: 177.5479\n",
      "Epoch [660/7000], Loss: 175.9557\n",
      "Epoch [670/7000], Loss: 174.4362\n",
      "Epoch [680/7000], Loss: 172.9831\n",
      "Epoch [690/7000], Loss: 171.5910\n",
      "Epoch [700/7000], Loss: 170.2550\n",
      "Epoch [710/7000], Loss: 168.9717\n",
      "Epoch [720/7000], Loss: 167.7349\n",
      "Epoch [730/7000], Loss: 166.4577\n",
      "Epoch [740/7000], Loss: 165.1798\n",
      "Epoch [750/7000], Loss: 163.9406\n",
      "Epoch [760/7000], Loss: 162.7450\n",
      "Epoch [770/7000], Loss: 161.5910\n",
      "Epoch [780/7000], Loss: 160.4765\n",
      "Epoch [790/7000], Loss: 159.3976\n",
      "Epoch [800/7000], Loss: 158.3527\n",
      "Epoch [810/7000], Loss: 157.3391\n",
      "Epoch [820/7000], Loss: 156.3547\n",
      "Epoch [830/7000], Loss: 155.3970\n",
      "Epoch [840/7000], Loss: 154.4644\n",
      "Epoch [850/7000], Loss: 153.5302\n",
      "Epoch [860/7000], Loss: 152.0293\n",
      "Epoch [870/7000], Loss: 150.7047\n",
      "Epoch [880/7000], Loss: 149.6321\n",
      "Epoch [890/7000], Loss: 148.6701\n",
      "Epoch [900/7000], Loss: 147.7955\n",
      "Epoch [910/7000], Loss: 146.9876\n",
      "Epoch [920/7000], Loss: 146.2248\n",
      "Epoch [930/7000], Loss: 145.4966\n",
      "Epoch [940/7000], Loss: 144.7984\n",
      "Epoch [950/7000], Loss: 144.1259\n",
      "Epoch [960/7000], Loss: 143.4761\n",
      "Epoch [970/7000], Loss: 142.8446\n",
      "Epoch [980/7000], Loss: 142.2286\n",
      "Epoch [990/7000], Loss: 141.6326\n",
      "Epoch [1000/7000], Loss: 141.0517\n",
      "Epoch [1010/7000], Loss: 140.4853\n",
      "Epoch [1020/7000], Loss: 139.9317\n",
      "Epoch [1030/7000], Loss: 139.3900\n",
      "Epoch [1040/7000], Loss: 138.8586\n",
      "Epoch [1050/7000], Loss: 138.3358\n",
      "Epoch [1060/7000], Loss: 137.8208\n",
      "Epoch [1070/7000], Loss: 137.3117\n",
      "Epoch [1080/7000], Loss: 136.7748\n",
      "Epoch [1090/7000], Loss: 136.2364\n",
      "Epoch [1100/7000], Loss: 135.7096\n",
      "Epoch [1110/7000], Loss: 135.1898\n",
      "Epoch [1120/7000], Loss: 134.6913\n",
      "Epoch [1130/7000], Loss: 134.2056\n",
      "Epoch [1140/7000], Loss: 133.7287\n",
      "Epoch [1150/7000], Loss: 133.2423\n",
      "Epoch [1160/7000], Loss: 132.9419\n",
      "Epoch [1170/7000], Loss: 132.3836\n",
      "Epoch [1180/7000], Loss: 132.1263\n",
      "Epoch [1190/7000], Loss: 131.4135\n",
      "Epoch [1200/7000], Loss: 130.8199\n",
      "Epoch [1210/7000], Loss: 133.7890\n",
      "Epoch [1220/7000], Loss: 163.7386\n",
      "Epoch [1230/7000], Loss: 145.6872\n",
      "Epoch [1240/7000], Loss: 130.2141\n",
      "Epoch [1250/7000], Loss: 131.6801\n",
      "Epoch [1260/7000], Loss: 129.3667\n",
      "Epoch [1270/7000], Loss: 128.5403\n",
      "Epoch [1280/7000], Loss: 128.1969\n",
      "Epoch [1290/7000], Loss: 127.7793\n",
      "Epoch [1300/7000], Loss: 127.3655\n",
      "Epoch [1310/7000], Loss: 126.9762\n",
      "Epoch [1320/7000], Loss: 126.5900\n",
      "Epoch [1330/7000], Loss: 126.2058\n",
      "Epoch [1340/7000], Loss: 125.8334\n",
      "Epoch [1350/7000], Loss: 125.8413\n",
      "Epoch [1360/7000], Loss: 163.6363\n",
      "Epoch [1370/7000], Loss: 6287.5200\n",
      "Epoch [1380/7000], Loss: 277.7378\n",
      "Epoch [1390/7000], Loss: 901.2855\n",
      "Epoch [1400/7000], Loss: 404.5447\n",
      "Epoch [1410/7000], Loss: 219.3613\n",
      "Epoch [1420/7000], Loss: 158.7746\n",
      "Epoch [1430/7000], Loss: 138.1682\n",
      "Epoch [1440/7000], Loss: 130.0131\n",
      "Epoch [1450/7000], Loss: 126.0446\n",
      "Epoch [1460/7000], Loss: 123.9685\n",
      "Epoch [1470/7000], Loss: 122.9105\n",
      "Epoch [1480/7000], Loss: 122.3076\n",
      "Epoch [1490/7000], Loss: 121.7842\n",
      "Epoch [1500/7000], Loss: 121.2849\n",
      "Epoch [1510/7000], Loss: 120.8425\n",
      "Epoch [1520/7000], Loss: 120.4186\n",
      "Epoch [1530/7000], Loss: 120.0111\n",
      "Epoch [1540/7000], Loss: 119.6216\n",
      "Epoch [1550/7000], Loss: 119.2454\n",
      "Epoch [1560/7000], Loss: 118.8769\n",
      "Epoch [1570/7000], Loss: 118.5160\n",
      "Epoch [1580/7000], Loss: 118.1628\n",
      "Epoch [1590/7000], Loss: 117.8139\n",
      "Epoch [1600/7000], Loss: 117.4741\n",
      "Epoch [1610/7000], Loss: 117.1465\n",
      "Epoch [1620/7000], Loss: 116.8239\n",
      "Epoch [1630/7000], Loss: 116.5067\n",
      "Epoch [1640/7000], Loss: 116.1957\n",
      "Epoch [1650/7000], Loss: 115.8886\n",
      "Epoch [1660/7000], Loss: 115.5846\n",
      "Epoch [1670/7000], Loss: 115.2830\n",
      "Epoch [1680/7000], Loss: 114.9858\n",
      "Epoch [1690/7000], Loss: 114.6954\n",
      "Epoch [1700/7000], Loss: 114.4092\n",
      "Epoch [1710/7000], Loss: 114.1260\n",
      "Epoch [1720/7000], Loss: 113.8508\n",
      "Epoch [1730/7000], Loss: 113.6298\n",
      "Epoch [1740/7000], Loss: 113.3321\n",
      "Epoch [1750/7000], Loss: 113.0905\n",
      "Epoch [1760/7000], Loss: 112.8043\n",
      "Epoch [1770/7000], Loss: 112.5478\n",
      "Epoch [1780/7000], Loss: 9115.9062\n",
      "Epoch [1790/7000], Loss: 1866.7240\n",
      "Epoch [1800/7000], Loss: 466.1703\n",
      "Epoch [1810/7000], Loss: 279.6217\n",
      "Epoch [1820/7000], Loss: 178.7888\n",
      "Epoch [1830/7000], Loss: 134.1803\n",
      "Epoch [1840/7000], Loss: 120.8760\n",
      "Epoch [1850/7000], Loss: 117.2318\n",
      "Epoch [1860/7000], Loss: 115.9263\n",
      "Epoch [1870/7000], Loss: 115.2006\n",
      "Epoch [1880/7000], Loss: 114.6890\n",
      "Epoch [1890/7000], Loss: 114.2631\n",
      "Epoch [1900/7000], Loss: 113.8897\n",
      "Epoch [1910/7000], Loss: 113.5739\n",
      "Epoch [1920/7000], Loss: 113.2931\n",
      "Epoch [1930/7000], Loss: 113.0296\n",
      "Epoch [1940/7000], Loss: 112.7751\n",
      "Epoch [1950/7000], Loss: 112.5283\n",
      "Epoch [1960/7000], Loss: 112.2879\n",
      "Epoch [1970/7000], Loss: 112.0523\n",
      "Epoch [1980/7000], Loss: 111.8238\n",
      "Epoch [1990/7000], Loss: 111.5992\n",
      "Epoch [2000/7000], Loss: 111.3775\n",
      "Epoch [2010/7000], Loss: 111.1598\n",
      "Epoch [2020/7000], Loss: 110.9424\n",
      "Epoch [2030/7000], Loss: 110.7316\n",
      "Epoch [2040/7000], Loss: 110.5258\n",
      "Epoch [2050/7000], Loss: 110.3246\n",
      "Epoch [2060/7000], Loss: 110.1261\n",
      "Epoch [2070/7000], Loss: 109.9260\n",
      "Epoch [2080/7000], Loss: 109.7250\n",
      "Epoch [2090/7000], Loss: 109.5277\n",
      "Epoch [2100/7000], Loss: 109.3365\n",
      "Epoch [2110/7000], Loss: 109.1498\n",
      "Epoch [2120/7000], Loss: 108.9671\n",
      "Epoch [2130/7000], Loss: 108.7883\n",
      "Epoch [2140/7000], Loss: 108.6122\n",
      "Epoch [2150/7000], Loss: 108.4379\n",
      "Epoch [2160/7000], Loss: 108.2626\n",
      "Epoch [2170/7000], Loss: 108.0842\n",
      "Epoch [2180/7000], Loss: 107.9064\n",
      "Epoch [2190/7000], Loss: 107.7266\n",
      "Epoch [2200/7000], Loss: 107.5474\n",
      "Epoch [2210/7000], Loss: 107.3696\n",
      "Epoch [2220/7000], Loss: 107.1941\n",
      "Epoch [2230/7000], Loss: 107.0200\n",
      "Epoch [2240/7000], Loss: 106.8482\n",
      "Epoch [2250/7000], Loss: 106.7132\n",
      "Epoch [2260/7000], Loss: 110.5775\n",
      "Epoch [2270/7000], Loss: 925.9415\n",
      "Epoch [2280/7000], Loss: 917.5430\n",
      "Epoch [2290/7000], Loss: 329.7612\n",
      "Epoch [2300/7000], Loss: 268.4674\n",
      "Epoch [2310/7000], Loss: 217.2375\n",
      "Epoch [2320/7000], Loss: 149.0808\n",
      "Epoch [2330/7000], Loss: 119.6912\n",
      "Epoch [2340/7000], Loss: 111.1751\n",
      "Epoch [2350/7000], Loss: 110.0760\n",
      "Epoch [2360/7000], Loss: 109.5713\n",
      "Epoch [2370/7000], Loss: 108.6396\n",
      "Epoch [2380/7000], Loss: 108.0968\n",
      "Epoch [2390/7000], Loss: 107.7794\n",
      "Epoch [2400/7000], Loss: 107.4622\n",
      "Epoch [2410/7000], Loss: 107.1945\n",
      "Epoch [2420/7000], Loss: 106.9451\n",
      "Epoch [2430/7000], Loss: 106.7138\n",
      "Epoch [2440/7000], Loss: 106.4973\n",
      "Epoch [2450/7000], Loss: 106.2908\n",
      "Epoch [2460/7000], Loss: 106.0915\n",
      "Epoch [2470/7000], Loss: 105.8989\n",
      "Epoch [2480/7000], Loss: 105.7116\n",
      "Epoch [2490/7000], Loss: 105.5283\n",
      "Epoch [2500/7000], Loss: 105.3483\n",
      "Epoch [2510/7000], Loss: 105.1744\n",
      "Epoch [2520/7000], Loss: 105.0070\n",
      "Epoch [2530/7000], Loss: 104.8456\n",
      "Epoch [2540/7000], Loss: 104.6863\n",
      "Epoch [2550/7000], Loss: 104.5305\n",
      "Epoch [2560/7000], Loss: 104.3761\n",
      "Epoch [2570/7000], Loss: 104.2225\n",
      "Epoch [2580/7000], Loss: 104.0720\n",
      "Epoch [2590/7000], Loss: 103.9209\n",
      "Epoch [2600/7000], Loss: 103.7713\n",
      "Epoch [2610/7000], Loss: 103.6222\n",
      "Epoch [2620/7000], Loss: 103.4733\n",
      "Epoch [2630/7000], Loss: 103.3225\n",
      "Epoch [2640/7000], Loss: 103.1721\n",
      "Epoch [2650/7000], Loss: 103.0218\n",
      "Epoch [2660/7000], Loss: 102.8745\n",
      "Epoch [2670/7000], Loss: 102.7278\n",
      "Epoch [2680/7000], Loss: 102.5806\n",
      "Epoch [2690/7000], Loss: 102.4357\n",
      "Epoch [2700/7000], Loss: 102.2927\n",
      "Epoch [2710/7000], Loss: 102.1507\n",
      "Epoch [2720/7000], Loss: 102.0102\n",
      "Epoch [2730/7000], Loss: 101.8697\n",
      "Epoch [2740/7000], Loss: 101.7311\n",
      "Epoch [2750/7000], Loss: 101.5956\n",
      "Epoch [2760/7000], Loss: 101.4648\n",
      "Epoch [2770/7000], Loss: 101.3353\n",
      "Epoch [2780/7000], Loss: 101.1989\n",
      "Epoch [2790/7000], Loss: 101.0495\n",
      "Epoch [2800/7000], Loss: 100.8985\n",
      "Epoch [2810/7000], Loss: 100.7483\n",
      "Epoch [2820/7000], Loss: 100.5990\n",
      "Epoch [2830/7000], Loss: 100.4470\n",
      "Epoch [2840/7000], Loss: 100.2935\n",
      "Epoch [2850/7000], Loss: 100.1402\n",
      "Epoch [2860/7000], Loss: 99.9810\n",
      "Epoch [2870/7000], Loss: 99.8170\n",
      "Epoch [2880/7000], Loss: 99.6597\n",
      "Epoch [2890/7000], Loss: 99.5173\n",
      "Epoch [2900/7000], Loss: 99.3816\n",
      "Epoch [2910/7000], Loss: 99.2525\n",
      "Epoch [2920/7000], Loss: 99.1306\n",
      "Epoch [2930/7000], Loss: 99.0131\n",
      "Epoch [2940/7000], Loss: 98.9004\n",
      "Epoch [2950/7000], Loss: 98.7913\n",
      "Epoch [2960/7000], Loss: 98.7542\n",
      "Epoch [2970/7000], Loss: 106.6217\n",
      "Epoch [2980/7000], Loss: 1676.3634\n",
      "Epoch [2990/7000], Loss: 1269.1302\n",
      "Epoch [3000/7000], Loss: 159.0407\n",
      "Epoch [3010/7000], Loss: 116.1933\n",
      "Epoch [3020/7000], Loss: 111.5010\n",
      "Epoch [3030/7000], Loss: 109.0482\n",
      "Epoch [3040/7000], Loss: 106.7443\n",
      "Epoch [3050/7000], Loss: 104.8267\n",
      "Epoch [3060/7000], Loss: 103.6251\n",
      "Epoch [3070/7000], Loss: 102.9341\n",
      "Epoch [3080/7000], Loss: 102.5206\n",
      "Epoch [3090/7000], Loss: 102.1983\n",
      "Epoch [3100/7000], Loss: 101.9036\n",
      "Epoch [3110/7000], Loss: 101.6341\n",
      "Epoch [3120/7000], Loss: 101.3922\n",
      "Epoch [3130/7000], Loss: 101.1634\n",
      "Epoch [3140/7000], Loss: 100.9388\n",
      "Epoch [3150/7000], Loss: 100.7315\n",
      "Epoch [3160/7000], Loss: 100.5416\n",
      "Epoch [3170/7000], Loss: 100.3634\n",
      "Epoch [3180/7000], Loss: 100.1937\n",
      "Epoch [3190/7000], Loss: 100.0333\n",
      "Epoch [3200/7000], Loss: 99.8837\n",
      "Epoch [3210/7000], Loss: 99.7401\n",
      "Epoch [3220/7000], Loss: 99.6022\n",
      "Epoch [3230/7000], Loss: 99.4709\n",
      "Epoch [3240/7000], Loss: 99.3427\n",
      "Epoch [3250/7000], Loss: 99.2148\n",
      "Epoch [3260/7000], Loss: 99.0891\n",
      "Epoch [3270/7000], Loss: 98.9660\n",
      "Epoch [3280/7000], Loss: 98.8454\n",
      "Epoch [3290/7000], Loss: 98.7252\n",
      "Epoch [3300/7000], Loss: 98.6043\n",
      "Epoch [3310/7000], Loss: 98.4847\n",
      "Epoch [3320/7000], Loss: 98.3652\n",
      "Epoch [3330/7000], Loss: 98.2471\n",
      "Epoch [3340/7000], Loss: 98.1303\n",
      "Epoch [3350/7000], Loss: 98.0167\n",
      "Epoch [3360/7000], Loss: 97.9042\n",
      "Epoch [3370/7000], Loss: 97.7935\n",
      "Epoch [3380/7000], Loss: 97.6826\n",
      "Epoch [3390/7000], Loss: 97.5733\n",
      "Epoch [3400/7000], Loss: 97.4663\n",
      "Epoch [3410/7000], Loss: 97.3606\n",
      "Epoch [3420/7000], Loss: 97.2553\n",
      "Epoch [3430/7000], Loss: 97.1511\n",
      "Epoch [3440/7000], Loss: 97.0468\n",
      "Epoch [3450/7000], Loss: 96.9428\n",
      "Epoch [3460/7000], Loss: 96.8412\n",
      "Epoch [3470/7000], Loss: 96.7406\n",
      "Epoch [3480/7000], Loss: 96.6414\n",
      "Epoch [3490/7000], Loss: 96.5431\n",
      "Epoch [3500/7000], Loss: 96.4452\n",
      "Epoch [3510/7000], Loss: 96.3490\n",
      "Epoch [3520/7000], Loss: 96.2523\n",
      "Epoch [3530/7000], Loss: 96.1549\n",
      "Epoch [3540/7000], Loss: 96.0584\n",
      "Epoch [3550/7000], Loss: 95.9622\n",
      "Epoch [3560/7000], Loss: 95.8681\n",
      "Epoch [3570/7000], Loss: 95.7733\n",
      "Epoch [3580/7000], Loss: 95.6799\n",
      "Epoch [3590/7000], Loss: 95.5868\n",
      "Epoch [3600/7000], Loss: 95.4945\n",
      "Epoch [3610/7000], Loss: 95.4021\n",
      "Epoch [3620/7000], Loss: 95.3106\n",
      "Epoch [3630/7000], Loss: 95.2194\n",
      "Epoch [3640/7000], Loss: 95.1290\n",
      "Epoch [3650/7000], Loss: 95.0394\n",
      "Epoch [3660/7000], Loss: 94.9512\n",
      "Epoch [3670/7000], Loss: 94.9155\n",
      "Epoch [3680/7000], Loss: 100.8796\n",
      "Epoch [3690/7000], Loss: 1379.0061\n",
      "Epoch [3700/7000], Loss: 190.7715\n",
      "Epoch [3710/7000], Loss: 395.8279\n",
      "Epoch [3720/7000], Loss: 198.0814\n",
      "Epoch [3730/7000], Loss: 124.1670\n",
      "Epoch [3740/7000], Loss: 104.0950\n",
      "Epoch [3750/7000], Loss: 102.0648\n",
      "Epoch [3760/7000], Loss: 101.1743\n",
      "Epoch [3770/7000], Loss: 100.2768\n",
      "Epoch [3780/7000], Loss: 99.6474\n",
      "Epoch [3790/7000], Loss: 99.2052\n",
      "Epoch [3800/7000], Loss: 98.8585\n",
      "Epoch [3810/7000], Loss: 98.5795\n",
      "Epoch [3820/7000], Loss: 98.3393\n",
      "Epoch [3830/7000], Loss: 98.1199\n",
      "Epoch [3840/7000], Loss: 97.9161\n",
      "Epoch [3850/7000], Loss: 97.7281\n",
      "Epoch [3860/7000], Loss: 97.5477\n",
      "Epoch [3870/7000], Loss: 97.3721\n",
      "Epoch [3880/7000], Loss: 97.2023\n",
      "Epoch [3890/7000], Loss: 97.0359\n",
      "Epoch [3900/7000], Loss: 96.8767\n",
      "Epoch [3910/7000], Loss: 96.7240\n",
      "Epoch [3920/7000], Loss: 96.5743\n",
      "Epoch [3930/7000], Loss: 96.4303\n",
      "Epoch [3940/7000], Loss: 96.2843\n",
      "Epoch [3950/7000], Loss: 96.1415\n",
      "Epoch [3960/7000], Loss: 96.0052\n",
      "Epoch [3970/7000], Loss: 95.8708\n",
      "Epoch [3980/7000], Loss: 95.7386\n",
      "Epoch [3990/7000], Loss: 95.6126\n",
      "Epoch [4000/7000], Loss: 95.4896\n",
      "Epoch [4010/7000], Loss: 95.3703\n",
      "Epoch [4020/7000], Loss: 95.2550\n",
      "Epoch [4030/7000], Loss: 95.1434\n",
      "Epoch [4040/7000], Loss: 95.0349\n",
      "Epoch [4050/7000], Loss: 94.9275\n",
      "Epoch [4060/7000], Loss: 94.8204\n",
      "Epoch [4070/7000], Loss: 94.7159\n",
      "Epoch [4080/7000], Loss: 94.6116\n",
      "Epoch [4090/7000], Loss: 94.5079\n",
      "Epoch [4100/7000], Loss: 94.4058\n",
      "Epoch [4110/7000], Loss: 94.3046\n",
      "Epoch [4120/7000], Loss: 94.2022\n",
      "Epoch [4130/7000], Loss: 94.1016\n",
      "Epoch [4140/7000], Loss: 94.0018\n",
      "Epoch [4150/7000], Loss: 93.9025\n",
      "Epoch [4160/7000], Loss: 93.8044\n",
      "Epoch [4170/7000], Loss: 93.7081\n",
      "Epoch [4180/7000], Loss: 93.6129\n",
      "Epoch [4190/7000], Loss: 93.5165\n",
      "Epoch [4200/7000], Loss: 93.4228\n",
      "Epoch [4210/7000], Loss: 93.3291\n",
      "Epoch [4220/7000], Loss: 93.2371\n",
      "Epoch [4230/7000], Loss: 93.1468\n",
      "Epoch [4240/7000], Loss: 93.0561\n",
      "Epoch [4250/7000], Loss: 92.9670\n",
      "Epoch [4260/7000], Loss: 92.8788\n",
      "Epoch [4270/7000], Loss: 92.7917\n",
      "Epoch [4280/7000], Loss: 92.7039\n",
      "Epoch [4290/7000], Loss: 92.6177\n",
      "Epoch [4300/7000], Loss: 92.5322\n",
      "Epoch [4310/7000], Loss: 92.4473\n",
      "Epoch [4320/7000], Loss: 92.3638\n",
      "Epoch [4330/7000], Loss: 92.2944\n",
      "Epoch [4340/7000], Loss: 93.7979\n",
      "Epoch [4350/7000], Loss: 524.3256\n",
      "Epoch [4360/7000], Loss: 795.9761\n",
      "Epoch [4370/7000], Loss: 324.1003\n",
      "Epoch [4380/7000], Loss: 137.9798\n",
      "Epoch [4390/7000], Loss: 108.3800\n",
      "Epoch [4400/7000], Loss: 107.7654\n",
      "Epoch [4410/7000], Loss: 103.2289\n",
      "Epoch [4420/7000], Loss: 98.8664\n",
      "Epoch [4430/7000], Loss: 98.0786\n",
      "Epoch [4440/7000], Loss: 97.3023\n",
      "Epoch [4450/7000], Loss: 96.7749\n",
      "Epoch [4460/7000], Loss: 96.3757\n",
      "Epoch [4470/7000], Loss: 96.0254\n",
      "Epoch [4480/7000], Loss: 95.7284\n",
      "Epoch [4490/7000], Loss: 95.4727\n",
      "Epoch [4500/7000], Loss: 95.2330\n",
      "Epoch [4510/7000], Loss: 94.9981\n",
      "Epoch [4520/7000], Loss: 94.7649\n",
      "Epoch [4530/7000], Loss: 94.5443\n",
      "Epoch [4540/7000], Loss: 94.3378\n",
      "Epoch [4550/7000], Loss: 94.1414\n",
      "Epoch [4560/7000], Loss: 93.9576\n",
      "Epoch [4570/7000], Loss: 93.7839\n",
      "Epoch [4580/7000], Loss: 93.6163\n",
      "Epoch [4590/7000], Loss: 93.4554\n",
      "Epoch [4600/7000], Loss: 93.2984\n",
      "Epoch [4610/7000], Loss: 93.1448\n",
      "Epoch [4620/7000], Loss: 92.9953\n",
      "Epoch [4630/7000], Loss: 92.8480\n",
      "Epoch [4640/7000], Loss: 92.7016\n",
      "Epoch [4650/7000], Loss: 92.5585\n",
      "Epoch [4660/7000], Loss: 92.4220\n",
      "Epoch [4670/7000], Loss: 92.2861\n",
      "Epoch [4680/7000], Loss: 92.1541\n",
      "Epoch [4690/7000], Loss: 92.0256\n",
      "Epoch [4700/7000], Loss: 91.8980\n",
      "Epoch [4710/7000], Loss: 91.7738\n",
      "Epoch [4720/7000], Loss: 91.6525\n",
      "Epoch [4730/7000], Loss: 91.5328\n",
      "Epoch [4740/7000], Loss: 91.4171\n",
      "Epoch [4750/7000], Loss: 91.3026\n",
      "Epoch [4760/7000], Loss: 91.1901\n",
      "Epoch [4770/7000], Loss: 91.0804\n",
      "Epoch [4780/7000], Loss: 90.9734\n",
      "Epoch [4790/7000], Loss: 90.8686\n",
      "Epoch [4800/7000], Loss: 90.7651\n",
      "Epoch [4810/7000], Loss: 90.6639\n",
      "Epoch [4820/7000], Loss: 90.5647\n",
      "Epoch [4830/7000], Loss: 90.4664\n",
      "Epoch [4840/7000], Loss: 90.3693\n",
      "Epoch [4850/7000], Loss: 90.2736\n",
      "Epoch [4860/7000], Loss: 90.1799\n",
      "Epoch [4870/7000], Loss: 90.0871\n",
      "Epoch [4880/7000], Loss: 89.9947\n",
      "Epoch [4890/7000], Loss: 89.9025\n",
      "Epoch [4900/7000], Loss: 89.8134\n",
      "Epoch [4910/7000], Loss: 89.7220\n",
      "Epoch [4920/7000], Loss: 89.6333\n",
      "Epoch [4930/7000], Loss: 89.5470\n",
      "Epoch [4940/7000], Loss: 89.4600\n",
      "Epoch [4950/7000], Loss: 89.3769\n",
      "Epoch [4960/7000], Loss: 89.2940\n",
      "Epoch [4970/7000], Loss: 89.2121\n",
      "Epoch [4980/7000], Loss: 89.1308\n",
      "Epoch [4990/7000], Loss: 89.0499\n",
      "Epoch [5000/7000], Loss: 88.9696\n",
      "Epoch [5010/7000], Loss: 88.8907\n",
      "Epoch [5020/7000], Loss: 88.8119\n",
      "Epoch [5030/7000], Loss: 88.7354\n",
      "Epoch [5040/7000], Loss: 88.6615\n",
      "Epoch [5050/7000], Loss: 88.5872\n",
      "Epoch [5060/7000], Loss: 88.5125\n",
      "Epoch [5070/7000], Loss: 88.4383\n",
      "Epoch [5080/7000], Loss: 88.3651\n",
      "Epoch [5090/7000], Loss: 88.2920\n",
      "Epoch [5100/7000], Loss: 88.2193\n",
      "Epoch [5110/7000], Loss: 88.1526\n",
      "Epoch [5120/7000], Loss: 88.3791\n",
      "Epoch [5130/7000], Loss: 154.0252\n",
      "Epoch [5140/7000], Loss: 146.7179\n",
      "Epoch [5150/7000], Loss: 191.9601\n",
      "Epoch [5160/7000], Loss: 136.2157\n",
      "Epoch [5170/7000], Loss: 108.1805\n",
      "Epoch [5180/7000], Loss: 98.3665\n",
      "Epoch [5190/7000], Loss: 94.3305\n",
      "Epoch [5200/7000], Loss: 92.9943\n",
      "Epoch [5210/7000], Loss: 92.5724\n",
      "Epoch [5220/7000], Loss: 92.0694\n",
      "Epoch [5230/7000], Loss: 91.6777\n",
      "Epoch [5240/7000], Loss: 91.4077\n",
      "Epoch [5250/7000], Loss: 91.1218\n",
      "Epoch [5260/7000], Loss: 90.8707\n",
      "Epoch [5270/7000], Loss: 90.6504\n",
      "Epoch [5280/7000], Loss: 90.4660\n",
      "Epoch [5290/7000], Loss: 90.3009\n",
      "Epoch [5300/7000], Loss: 90.1503\n",
      "Epoch [5310/7000], Loss: 90.0102\n",
      "Epoch [5320/7000], Loss: 89.8759\n",
      "Epoch [5330/7000], Loss: 89.7453\n",
      "Epoch [5340/7000], Loss: 89.6203\n",
      "Epoch [5350/7000], Loss: 89.4974\n",
      "Epoch [5360/7000], Loss: 89.3789\n",
      "Epoch [5370/7000], Loss: 89.2644\n",
      "Epoch [5380/7000], Loss: 89.1545\n",
      "Epoch [5390/7000], Loss: 89.0502\n",
      "Epoch [5400/7000], Loss: 88.9510\n",
      "Epoch [5410/7000], Loss: 88.8557\n",
      "Epoch [5420/7000], Loss: 88.7634\n",
      "Epoch [5430/7000], Loss: 88.6727\n",
      "Epoch [5440/7000], Loss: 88.5834\n",
      "Epoch [5450/7000], Loss: 88.4941\n",
      "Epoch [5460/7000], Loss: 88.4069\n",
      "Epoch [5470/7000], Loss: 88.3217\n",
      "Epoch [5480/7000], Loss: 88.2381\n",
      "Epoch [5490/7000], Loss: 88.1571\n",
      "Epoch [5500/7000], Loss: 88.0765\n",
      "Epoch [5510/7000], Loss: 87.9978\n",
      "Epoch [5520/7000], Loss: 87.9207\n",
      "Epoch [5530/7000], Loss: 87.8458\n",
      "Epoch [5540/7000], Loss: 87.7727\n",
      "Epoch [5550/7000], Loss: 87.7007\n",
      "Epoch [5560/7000], Loss: 87.6279\n",
      "Epoch [5570/7000], Loss: 87.5548\n",
      "Epoch [5580/7000], Loss: 87.4828\n",
      "Epoch [5590/7000], Loss: 87.4116\n",
      "Epoch [5600/7000], Loss: 87.3413\n",
      "Epoch [5610/7000], Loss: 87.2715\n",
      "Epoch [5620/7000], Loss: 87.2031\n",
      "Epoch [5630/7000], Loss: 87.1357\n",
      "Epoch [5640/7000], Loss: 87.0705\n",
      "Epoch [5650/7000], Loss: 87.0080\n",
      "Epoch [5660/7000], Loss: 86.9462\n",
      "Epoch [5670/7000], Loss: 86.8861\n",
      "Epoch [5680/7000], Loss: 86.8272\n",
      "Epoch [5690/7000], Loss: 86.7694\n",
      "Epoch [5700/7000], Loss: 86.7128\n",
      "Epoch [5710/7000], Loss: 86.6556\n",
      "Epoch [5720/7000], Loss: 86.6002\n",
      "Epoch [5730/7000], Loss: 86.5458\n",
      "Epoch [5740/7000], Loss: 86.4920\n",
      "Epoch [5750/7000], Loss: 86.4388\n",
      "Epoch [5760/7000], Loss: 86.3860\n",
      "Epoch [5770/7000], Loss: 86.3345\n",
      "Epoch [5780/7000], Loss: 86.2829\n",
      "Epoch [5790/7000], Loss: 86.2323\n",
      "Epoch [5800/7000], Loss: 86.1824\n",
      "Epoch [5810/7000], Loss: 86.1319\n",
      "Epoch [5820/7000], Loss: 86.0831\n",
      "Epoch [5830/7000], Loss: 86.0354\n",
      "Epoch [5840/7000], Loss: 86.0783\n",
      "Epoch [5850/7000], Loss: 113.1852\n",
      "Epoch [5860/7000], Loss: 193.2726\n",
      "Epoch [5870/7000], Loss: 164.6089\n",
      "Epoch [5880/7000], Loss: 136.7833\n",
      "Epoch [5890/7000], Loss: 101.3783\n",
      "Epoch [5900/7000], Loss: 97.6266\n",
      "Epoch [5910/7000], Loss: 92.1526\n",
      "Epoch [5920/7000], Loss: 91.2991\n",
      "Epoch [5930/7000], Loss: 90.6535\n",
      "Epoch [5940/7000], Loss: 90.1815\n",
      "Epoch [5950/7000], Loss: 89.8485\n",
      "Epoch [5960/7000], Loss: 89.5748\n",
      "Epoch [5970/7000], Loss: 89.3318\n",
      "Epoch [5980/7000], Loss: 89.1085\n",
      "Epoch [5990/7000], Loss: 88.9359\n",
      "Epoch [6000/7000], Loss: 88.7588\n",
      "Epoch [6010/7000], Loss: 88.5289\n",
      "Epoch [6020/7000], Loss: 88.3166\n",
      "Epoch [6030/7000], Loss: 88.1579\n",
      "Epoch [6040/7000], Loss: 88.0027\n",
      "Epoch [6050/7000], Loss: 87.8677\n",
      "Epoch [6060/7000], Loss: 87.7353\n",
      "Epoch [6070/7000], Loss: 87.5990\n",
      "Epoch [6080/7000], Loss: 87.4692\n",
      "Epoch [6090/7000], Loss: 87.3653\n",
      "Epoch [6100/7000], Loss: 87.2486\n",
      "Epoch [6110/7000], Loss: 87.1357\n",
      "Epoch [6120/7000], Loss: 87.0465\n",
      "Epoch [6130/7000], Loss: 86.9464\n",
      "Epoch [6140/7000], Loss: 86.8407\n",
      "Epoch [6150/7000], Loss: 86.7403\n",
      "Epoch [6160/7000], Loss: 86.6597\n",
      "Epoch [6170/7000], Loss: 86.5791\n",
      "Epoch [6180/7000], Loss: 86.4895\n",
      "Epoch [6190/7000], Loss: 86.4000\n",
      "Epoch [6200/7000], Loss: 86.3121\n",
      "Epoch [6210/7000], Loss: 86.2331\n",
      "Epoch [6220/7000], Loss: 86.1734\n",
      "Epoch [6230/7000], Loss: 86.0966\n",
      "Epoch [6240/7000], Loss: 86.0220\n",
      "Epoch [6250/7000], Loss: 85.9495\n",
      "Epoch [6260/7000], Loss: 85.8837\n",
      "Epoch [6270/7000], Loss: 85.8184\n",
      "Epoch [6280/7000], Loss: 85.7656\n",
      "Epoch [6290/7000], Loss: 85.6959\n",
      "Epoch [6300/7000], Loss: 85.6240\n",
      "Epoch [6310/7000], Loss: 85.5700\n",
      "Epoch [6320/7000], Loss: 85.5382\n",
      "Epoch [6330/7000], Loss: 85.4776\n",
      "Epoch [6340/7000], Loss: 85.4203\n",
      "Epoch [6350/7000], Loss: 85.3607\n",
      "Epoch [6360/7000], Loss: 85.3049\n",
      "Epoch [6370/7000], Loss: 85.2529\n",
      "Epoch [6380/7000], Loss: 85.2005\n",
      "Epoch [6390/7000], Loss: 85.1487\n",
      "Epoch [6400/7000], Loss: 85.0958\n",
      "Epoch [6410/7000], Loss: 85.0346\n",
      "Epoch [6420/7000], Loss: 84.9790\n",
      "Epoch [6430/7000], Loss: 84.9309\n",
      "Epoch [6440/7000], Loss: 84.8832\n",
      "Epoch [6450/7000], Loss: 84.8380\n",
      "Epoch [6460/7000], Loss: 84.7938\n",
      "Epoch [6470/7000], Loss: 84.7506\n",
      "Epoch [6480/7000], Loss: 84.7100\n",
      "Epoch [6490/7000], Loss: 84.6687\n",
      "Epoch [6500/7000], Loss: 84.6280\n",
      "Epoch [6510/7000], Loss: 84.5893\n",
      "Epoch [6520/7000], Loss: 84.5492\n",
      "Epoch [6530/7000], Loss: 84.5091\n",
      "Epoch [6540/7000], Loss: 84.4688\n",
      "Epoch [6550/7000], Loss: 84.4293\n",
      "Epoch [6560/7000], Loss: 84.3897\n",
      "Epoch [6570/7000], Loss: 84.3432\n",
      "Epoch [6580/7000], Loss: 88.8971\n",
      "Epoch [6590/7000], Loss: 599.9393\n",
      "Epoch [6600/7000], Loss: 108.0747\n",
      "Epoch [6610/7000], Loss: 97.1859\n",
      "Epoch [6620/7000], Loss: 94.0014\n",
      "Epoch [6630/7000], Loss: 90.9482\n",
      "Epoch [6640/7000], Loss: 88.2073\n",
      "Epoch [6650/7000], Loss: 87.5844\n",
      "Epoch [6660/7000], Loss: 87.1501\n",
      "Epoch [6670/7000], Loss: 86.7859\n",
      "Epoch [6680/7000], Loss: 86.5426\n",
      "Epoch [6690/7000], Loss: 86.3601\n",
      "Epoch [6700/7000], Loss: 86.2073\n",
      "Epoch [6710/7000], Loss: 86.0736\n",
      "Epoch [6720/7000], Loss: 85.9557\n",
      "Epoch [6730/7000], Loss: 85.8465\n",
      "Epoch [6740/7000], Loss: 85.7442\n",
      "Epoch [6750/7000], Loss: 85.6470\n",
      "Epoch [6760/7000], Loss: 85.5544\n",
      "Epoch [6770/7000], Loss: 85.4657\n",
      "Epoch [6780/7000], Loss: 85.3823\n",
      "Epoch [6790/7000], Loss: 85.3028\n",
      "Epoch [6800/7000], Loss: 85.2274\n",
      "Epoch [6810/7000], Loss: 85.1551\n",
      "Epoch [6820/7000], Loss: 85.0877\n",
      "Epoch [6830/7000], Loss: 85.0225\n",
      "Epoch [6840/7000], Loss: 84.9604\n",
      "Epoch [6850/7000], Loss: 84.9009\n",
      "Epoch [6860/7000], Loss: 84.8439\n",
      "Epoch [6870/7000], Loss: 84.7883\n",
      "Epoch [6880/7000], Loss: 84.7349\n",
      "Epoch [6890/7000], Loss: 84.6865\n",
      "Epoch [6900/7000], Loss: 84.6388\n",
      "Epoch [6910/7000], Loss: 84.5932\n",
      "Epoch [6920/7000], Loss: 84.5483\n",
      "Epoch [6930/7000], Loss: 84.5040\n",
      "Epoch [6940/7000], Loss: 84.4615\n",
      "Epoch [6950/7000], Loss: 84.4203\n",
      "Epoch [6960/7000], Loss: 84.3810\n",
      "Epoch [6970/7000], Loss: 84.3413\n",
      "Epoch [6980/7000], Loss: 84.3042\n",
      "Epoch [6990/7000], Loss: 84.2674\n",
      "Epoch [7000/7000], Loss: 84.2315\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 7000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to training mode (important if you have dropout or batchnorm)\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass\n",
    "    predictions = model(X_train_t).squeeze()  # shape: (train_size,)\n",
    "    loss = criterion(predictions, y_train_t)\n",
    "\n",
    "    # Backward pass & update\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print progress every few epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 111.9770\n",
      "Test RMSE: 10.5819\n"
     ]
    }
   ],
   "source": [
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():  # no gradient calculation needed\n",
    "    test_predictions = model(X_test_t).squeeze()\n",
    "    test_loss = criterion(test_predictions, y_test_t)\n",
    "\n",
    "rmse = torch.sqrt(test_loss)\n",
    "print(f\"Test MSE: {test_loss.item():.4f}\")\n",
    "print(f\"Test RMSE: {rmse.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
